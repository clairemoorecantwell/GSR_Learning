# class Lexicon:

#     def __init__(self, filename=None, sheetNo=1):
#         self.lexemeList = {}  # dict will have the structure {tag: lexeme}
#         self.freqList = []  # empty list for storing each lexeme's frequency - used for sampling
#         if filename is not None:
#             self.read_input(filename, sheetNo)

#     def add_lexeme(self, l):
#         # Add a lexeme to the lexemeList
#         self.lexemeList[l.tag] = l
#         self.freqList.append(l.freq)

#     def update_freqList(self):
#         # update freqList with all lexemes' frequencies, in case they have changed
#         self.freqList = []
#         for l in self.lexemeList:
#             self.freqList.append(l.freq)

#     def read_input(self, file_name, sheet_number):  # sheet_number starts from 1
#         # TODO add functionality to check if its an excel file or .txt; and then also be able to read .txt
#         # TODO further, add try... except to check for proper loading of pandas, and if not, throw up a useful error
#         # reading excel into dataframe
#         input_dt_frame = read_excel(file_name, sheet_number - 1)  # this is for the sheet#1;
#         # dataframe into np array
#         input_np = input_dt_frame.to_numpy()

#         cpyarr = input_np.tolist()
#         roots = []  # |
#         suffix = []
#         suffix_helper = []  # |
#         them_c_helper = []
#         them_c = []
#         allomorphs = []

#         # thematic c =(['t','n','f','g','m','l','s','q']) Note of list of all the thematic C's
#         for x in range(
#                 len(cpyarr)):  # looping through Julia's numpy array and separating roots, suffix, and them_c columns
#             roots.append(cpyarr[x][6])
#             suffix_helper.append(cpyarr[x][3])
#             them_c_helper.append(cpyarr[x][2])

#         suffix = list(set(suffix_helper))
#         them_c = list(set(them_c_helper))
#         them_c = list(filter(lambda x: str(x) != 'nan', them_c))  # to not generate something like "tagiNA"

#         for x in range(len(roots)):  # looping through roots column
#             allomorphs = []
#             for y in them_c:  # concat each root with all possible them_c
#                 allomorphs.append([roots[x] + y] + [0])
#             self.add_lexeme(lexeme(roots[x], allomorphs, 'root'))

#         tag = 'ina'
#         allomorphs = [['ina', 0], ['a', 0]]
#         for y in them_c:
#             allomorphs.append([y + 'ia'] + [0])
#         self.add_lexeme(lexeme(tag, allomorphs, 'suffix'))

#         tag = 'aga'
#         allomorphs = [['ga', 0]]
#         for y in them_c:
#             allomorphs.append([y + 'aga'] + [0])
#         self.add_lexeme(lexeme(tag, allomorphs, 'suffix'))

#         tag = 'aqi'
#         allomorphs = []
#         for y in them_c:
#             allomorphs.append([y + 'aqi'] + [0])
#         self.add_lexeme(lexeme(tag, allomorphs, 'suffix'))

#         tag = 'i'
#         allomorphs = []
#         for y in them_c:
#             allomorphs.append([y + 'i'] + [0])
#         self.add_lexeme(lexeme(tag, allomorphs, 'suffix'))

#         tag = 'zero'
#         allomorphs = ['']
#         self.add_lexeme(lexeme(tag, allomorphs, 'suffix'))




# class Tableaux:
#     def __init__(self, filename, noisy=False):
#         self.tableaux = []  # list for tableaux to go into
#         self.hidden = False
#         self.tabProb = False
#         self.constraintNames = []
#         self.read(filename, noisy)

#     def read(self, filename, noisy=True):
#         print('reading in tableaux from file...')
#         with open(filename, "r") as f:
#             lines = f.readlines()

#         # parse what kind of file this is
#         line1 = lines.pop(0)
#         line1 = line1.split('\t')
#         line1 = [label.strip() for label in
#                  line1]  # strip out leading and trailing whitespace for each entry in header row

#         if line1[0] != 'input':
#             print("WARNING: your first column is not labelled 'input' ... treating as input anyway")
#         if line1[1] != 'candidate':
#             print("WARNING: your second column is not labelled 'candidate' ... treating as candidate anyway")
#         if line1[2] != 'obs.prob':
#             if line1[2] != 'surface':
#                 try:
#                     float(lines[0].split('\t')[2])
#                     print(
#                         "WARNING: your third column is not labelled either 'obs.prob' or 'surface' ... column can be treated as float, so I'm assuming it's supposed to be obs.prob")
#                 except ValueError:
#                     print(
#                         "WARNING: your third column is not labelled either 'obs.prob' or 'surface' ... column cannot be treated as float, so I'm assuming it's supposed to be surface")
#                     self.hidden = True
#                     print("~~~~~~~~~~~ Hidden Structure is active ~~~~~~~~~~~~")
#             else:
#                 print("Third column is labelled 'surface'")
#                 self.hidden = True
#                 print("~~~~~~~~~~~ Hidden Structure is active ~~~~~~~~~~~~")
#                 if line1[3] != 'obs.prob':
#                     try:
#                         float(lines[0].split('\t')[2])
#                         print(
#                             "WARNING: your third column is labelled 'surface' but your fourth column is not labelled 'obs'prob' ... attempting to treat as obs.prob anyway")
#                     except ValueError:
#                         print(
#                             "ERROR: your third columns is labelled 'surface' but your fourth column is not labeleld 'obs.prob', and cannot be converted to float.  Exiting...")
#                         return
#         offset = 1 if self.hidden else 0
#         if line1[3 + offset] != 'tab.prob':
#             print("No 'tab.prob' column ... assuming all tableaux should be equally probable")
#         else:
#             self.tabProb = True

#         self.constraintNames = line1[4 + offset:]

#         # read in all the lines
#         # create a new tableau when the input changes
#         inpt = ''
#         for line in lines:
#             l = [entry.strip() for entry in line.split('\t')]
#             p = l[3 + offset] if self.tabProb else 1
#             if l[0] != inpt:  # if it's a new input - inputs have to be contiguous in the input file
#                 inpt = l[0]
#                 self.tableaux.append(Tableau(l[0], p))
#                 self.tableaux[-1].constraintNames = self.constraintNames

#             s = l[2] if self.hidden else None
#             self.tableaux[-1].addCandidate(candidate(l[1], [float(i) for i in l[4 + offset:]], float(l[2 + offset]), s))
#             if p != self.tableaux[-1].prob:
#                 print("WARNING: not all tab.prob entries for tableau " + self.tableaux[
#                     -1].tag + " are equal.  ...using the first one.")
#             # check if all tab.prob entries are the same for a given input - if not print a warning

#         if noisy:
#             for t in self.tableaux:
#                 print(t)

#         wellFormed = True
#         for t in self.tableaux:
#             wellFormed = True if t.rect(userChoice=True) else False
#         # Run tableau rectification on all tableau
#         # Other checks

#         return wellFormed  # return a bool for whether all tableaux are well formed or not






                 for u in [j for j in range(0, len(self.w)) if updateVector[j] != 0]:  # go through constraints that matter
                     # u is an index, into constraints and violations, and indexation values on lexemes
                     weights = self.lexCs[u][1:]  # vector of indexed C's for that constraint; empty if there are none
                     if weights and updateVector[u] > 0:
                         mx = max(weights)
                         mn = False
                     elif weights and updateVector[u] < 0:
                         mn = min(weights)
                         mx = False
                     else:
                         mn = False
                         mx = False

                     obsParsed = obs.c.split("_")
                     predParsed = pred.c.split("_")
                     if len(obsParsed) != len(predParsed) or len(obsParsed) != len(datum[0]):
                         print("predicted: " + pred.c)
                         print("observed: " + obs.c)
                         print(datum[0])
                         print(
                             Fore.RED + "\nERROR: predicted and/or observed cannot be aligned with lexemes - LexC induction will not work"+ Style.RESET_ALL)
                         exit

                     updateIndices = []
                     for i in range(0, len(datum[0])):
                         # find the lexeme that differs between obs and pred
                         # and update it and adjacent lexemes only
                         if obsParsed[i] != predParsed[i] and i not in updateIndices:
                             updateIndices.append(i)
                         #if i > 0 and i - 1 not in updateIndices:  # if we're not already updating the previous lexeme
                         #    updateIndices.append(i - 1)
                         #if i < len(datum[0]) - 1 and i + 1 not in updateIndices:
                         #    updateIndices.append(i + 1)

                     # now updateIndices is just a list of indices of lexemes which need to be updated
                     for i in updateIndices:
                         lex = datum[0][i]
                         currentIndex = lex.lexCindexes[u] - 1  # they start from 1
                         w = weights[currentIndex] if weights else 0  # current weight of the lexC indexed to this lexeme
                         if w == 0:  # if a lexC has decayed to zero, it counts as not-existent and is removed from the lexeme
                             lex.lexCindexes[u] = 0
                             currentIndex = -1
                         # print("re-indexed to 0")
                         if random.random() < self.pChangeIndexation:  # yes, we change indexation
                             # print("change Indexation...")
                             # the indexed C exists AND it is not the min/max already
                             # if currentIndex is not -1, then weights should not be []
                             # TODO add error-checking for this^

                             if currentIndex >= 0 and w != mx and w != mn:
                                 # note that whichever (max or min) is not applicable will equal False, not a number
                                 # we know we need to switch indexation

                                 s = sorted(weights[:])
                                 currentSpotInSort = s.index(w)
                                 if mx:  # obs preferring; move to next greater weight
                                     direction = 1
                                 else:  # pred preferring; move to next lower weight
                                     direction = -1

                                 move = 1
                                 while move:
                                     newWeight = s[currentSpotInSort + direction * move]
                                     if newWeight != w:
                                         move = 0
                                     else:
                                         move += 1

                                 # change the index on the lexeme to the location of that new weight
                                 lex.lexCindexes[u] = weights.index(
                                     newWeight) + 1  # plus 1 because we're indexing into the original lex weight vector
                             # print("re-indexed from "+str(currentIndex+1) + " to "+str(lex.lexCindexes[u]))
                             # re-indexed!

                             elif len([w for w in weights if
                                       w > 0]) < self.lexC_type and not mn:  # either the weight doesn't exist, or it's already at the min/max
                                 # only induce if at the max
                                 # still check whether we've already capped out our allowed number of weights
                                 # Induce at the first index that's 0, or append to end
                                 newIndex = 0 if 0 not in weights else weights.index(0) + 1
                                 if newIndex:  # we're re-populating a slot that got decayed to zero
                                     self.lexCs[u][newIndex] = self.lexCStartW
                                     lex.lexCindexes[u] = newIndex
                                 else:  # we're appending
                                     self.lexCs[u].append(self.lexCStartW)  # add a new lexC
                                     lex.lexCindexes[u] = len(self.lexCs[u]) - 1  # index to the new weight
                                 print("New cloned copy of " + self.trainingData.constraintNames[u])

                             elif currentIndex >= 0:  # Should only get here if we SHOULD induce but have already reached the max
                                 # just update
                                 # if weight needs to be lower than min, will just get updated down
                                 # don't proceed if there is no lexC affiliated with this lexeme
                                 self.lexCs[u][currentIndex + 1] += updateVector[u] * self.learningRate
                             # print("updating at index "+str(currentIndex+1))

                         else:  # we don't change indexation
                             if currentIndex >= 0 and weights and weights[currentIndex]:  # if lexC exists
                                 # update
                                 self.lexCs[u][currentIndex + 1] += updateVector[u] * self.learningRate
                             # print("updating at index "+str(currentIndex+1))

                     ##############################
                     # Decided not to do this part, because of Pater 2010
                     # Need lexemes to be able to dictate what happens outside their borders for Yine
                     # figure out which lexeme(s) differ
                     # only apply new clones to lexemes that differ
                     # parsed = obs.c.split("_")
                     # parsedPred = pred.c.split("_")
                     # if len(parsed)!=len(datum[0]) or len(parsedPred)!=len(datum[0]):
                     #  print(parsed)
                     #  print(datum[0])
                     #  print("ERROR: lexical indexation cannot be induced because morphemes in the candidate cannote be aligned with morphemes in the input")
                     #  exit
                     # for i in range(0,len(datum[0])):
                     #  if parsed[i]!=parsedPred[i]:
                     #      #Ok, now we will change the indexation of lexeme i
                     #      etcetera
                     # TODO possible way to split the difference on locality:
                     # find the lexeme(s) that differ from obs to pred, and only update those and adjacent lexemes

            ##################################################################

